{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3682e1-ef09-42aa-83dc-3473d64e9bc4",
     "showTitle": true,
     "title": "Importing the necessary libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import Window, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, udf, lit, col\n",
    "from pyspark_assert import assert_frame_equal\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "import functools\n",
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Started\n",
      "Code Executed Successfully\n"
     ]
    }
   ],
   "source": [
    "# Spark session initialization\n",
    "def create_spark_session():\n",
    "    \"\"\"Create a Spark Session\"\"\"\n",
    "    _ = load_dotenv()\n",
    "    return (\n",
    "        SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"SparkApp\")\n",
    "        .config(\"spark.driver.extraClassPath\", \"E:/App Dev/GitHub/backtesting_claude/.vscode/sqljdbc_12.8/enu/jars/mssql-jdbc-12.8.0.jre8.jar\") \\\n",
    "        .config(\"spark.executor.extraClassPath\", \"E:/App Dev/GitHub/backtesting_claude/.vscode/sqljdbc_12.8/enu/jars/mssql-jdbc-12.8.0.jre8.jar\") \\\n",
    "        .config(\"spark.jars\", \"E:/App Dev/GitHub/backtesting_claude/.vscode/sqljdbc_12.8/enu/jars/mssql-jdbc-12.8.0.jre8.jar\")\\\n",
    "        .config(\"spark.python.worker.memory\", \"8g\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\")\\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\")\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .getOrCreate()\n",
    "    )\n",
    "spark = create_spark_session()\n",
    "print('Session Started')\n",
    "print('Code Executed Successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection\n",
    "jdbcHostname = \"localhost\"\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"market_data_2\"\n",
    "stock = \"ACC\"\n",
    "jdbcTable = \"[dbo].[\" + stock + \"_master]\"\n",
    "jdbcTable_5 = \"[dbo].[\" + stock + \"_5min]\"\n",
    "jdbcTable_15 = \"[dbo].[\" + stock + \"_15min]\"\n",
    "jdbcTable_30 = \"[dbo].[\" + stock + \"_30min]\"\n",
    "jdbcTable_daily = \"[dbo].[\" + stock + \"_daily]\"\n",
    "jdbcTable_refLevels = \"[dbo].[\" + stock + \"_refLevels]\"\n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};database={jdbcDatabase};integratedSecurity=true;trustServerCertificate=true\"\n",
    "connectionProperties = {\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c82c56-f331-4a4a-b617-80edee4198b3",
     "showTitle": true,
     "title": "Custom Functions"
    }
   },
   "outputs": [],
   "source": [
    "def find_indexes(input_array, mid_index):\n",
    "    lower_side = input_array[:mid_index][::-1]\n",
    "    higher_side = input_array[mid_index + 1:]\n",
    "\n",
    "    total = input_array[mid_index]\n",
    "    overall_sum = sum(input_array)\n",
    "    idx_low = 0\n",
    "    idx_high = 0\n",
    "\n",
    "    while total < int(overall_sum * 0.7):\n",
    "        next_low = lower_side[idx_low] if idx_low < len(lower_side) else float('-inf')\n",
    "        next_high = higher_side[idx_high] if idx_high < len(higher_side) else float('-inf')\n",
    "\n",
    "        if next_low >= next_high:\n",
    "            total += next_low\n",
    "            idx_low += 1\n",
    "        else:\n",
    "            total += next_high\n",
    "            idx_high += 1\n",
    "\n",
    "    up_index = mid_index + idx_high + 1  # Adjust indices to get the final indexes\n",
    "    down_index = mid_index - idx_low\n",
    "    \n",
    "    return up_index, down_index\n",
    "\n",
    "\n",
    "@udf(\"array<float>\")\n",
    "def rep_(j_array, baseY, Den):\n",
    "    return [baseY + (i * Den) for i in (range(j_array + 1))]\n",
    "\n",
    "def calculateExtremes(input_array):\n",
    "    extremeCount = 0\n",
    "    for i in input_array:\n",
    "        if(i == 1 or i == 0):\n",
    "            extremeCount = extremeCount + 1\n",
    "        else:\n",
    "            break\n",
    "    return extremeCount\n",
    "\n",
    "\n",
    "def extremePresentCheck(buyExtremePresent, shortExtremePresent, netExtremeCount):\n",
    "    # 1-> Buy 2-> Short 3-> Neutral 4-> Absent\n",
    "    if buyExtremePresent == 0 and shortExtremePresent == 0:\n",
    "        return 4\n",
    "    elif (buyExtremePresent == 1 and shortExtremePresent == 0) or (buyExtremePresent == 1 and shortExtremePresent == 1 and netExtremeCount >= 3):\n",
    "        return 1\n",
    "    elif (buyExtremePresent == 0 and shortExtremePresent == 1) or (buyExtremePresent == 1 and shortExtremePresent == 1 and netExtremeCount <= -3):\n",
    "        return 2\n",
    "    elif (buyExtremePresent == 1 and shortExtremePresent == 1):\n",
    "        return 3 \n",
    "\n",
    "def rangeExtensionCheck(buyRangeExtension, shortRangeExtension):\n",
    "    # 1-> Buying 2-> Selling 3-> Neutral 4-> Absent\n",
    "    if buyRangeExtension == 0 and shortRangeExtension == 0:\n",
    "        return 4\n",
    "    elif buyRangeExtension == 1 and shortRangeExtension == 1:\n",
    "        return 3\n",
    "    elif buyRangeExtension == 1 and shortRangeExtension == 0:\n",
    "        return 1\n",
    "    elif buyRangeExtension == 0 and shortRangeExtension == 1:\n",
    "        return 2\n",
    "    \n",
    "def IBTypeCheck(IBRange, ATR):\n",
    "    # 1-> Small 2-> Normal 3-> Wide 4-> Very Wide\n",
    "    if IBRange <= 0.33 * ATR:\n",
    "        return 1\n",
    "    elif IBRange > 0.33 * ATR and IBRange <= 0.5 * ATR:\n",
    "        return 2\n",
    "    elif IBRange > 0.5 * ATR and IBRange <= 0.8 * ATR:\n",
    "        return 3\n",
    "    if IBRange > 0.8 * ATR:\n",
    "        return 4\n",
    "    \n",
    "def tpoCountCheck(buyTPOCount, shortTPOCount):\n",
    "    # 1-> Buying TPO 2-> Selling TPO 3-> Neutral TPO\n",
    "    if buyTPOCount == shortTPOCount:\n",
    "        return 3\n",
    "    elif buyTPOCount > shortTPOCount:\n",
    "        return 1\n",
    "    elif buyTPOCount < shortTPOCount:\n",
    "        return 2\n",
    "    \n",
    "def marketSentimentCheck(extremePresent, rangeExtension, tpoCount, valueShift):\n",
    "    # 1-> Positive 2-> Slightly Positive 3-> Negative 4-> Slightly Negative 5-> Neutral\n",
    "    if valueShift == 1 and rangeExtension == 1 and extremePresent == 1:\n",
    "        return 1\n",
    "    elif valueShift == 1 and rangeExtension == 4 and extremePresent == 4 and tpoCount == 1:\n",
    "        return 2\n",
    "    elif valueShift == 1 and rangeExtension == 1 and extremePresent != 1:\n",
    "        return 2\n",
    "    elif valueShift == 1 and rangeExtension != 1 and extremePresent == 1:\n",
    "        return 2\n",
    "    elif valueShift != 1 and rangeExtension == 1 and extremePresent == 1:\n",
    "        return 2\n",
    "    elif valueShift == 2 and rangeExtension == 2 and extremePresent == 2:\n",
    "        return 3\n",
    "    elif valueShift == 2 and rangeExtension == 4 and extremePresent == 4 and tpoCount == 2:\n",
    "        return 4\n",
    "    elif valueShift == 2 and rangeExtension == 2 and extremePresent != 2:\n",
    "        return 4\n",
    "    elif valueShift == 2 and rangeExtension != 2 and extremePresent == 2:\n",
    "        return 4\n",
    "    elif valueShift != 2 and rangeExtension == 2 and extremePresent == 2:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "def openLocationCheck(todayOpen, yestHigh, yestLow, vah_prev, val_prev):\n",
    "    # Within PDVA - 1, Above PDVA below PDH - 2, Above PDH - 3, Below PDVA above PDL - 4, Below PDL - 5  \n",
    "    if todayOpen <= vah_prev and todayOpen >= val_prev:\n",
    "        return 1\n",
    "    elif todayOpen > vah_prev and todayOpen <= yestHigh:\n",
    "        return 2\n",
    "    elif todayOpen > yestHigh:\n",
    "        return 3\n",
    "    elif todayOpen < val_prev and todayOpen >= yestLow:\n",
    "        return 4\n",
    "    elif todayOpen < yestLow:\n",
    "        return 5\n",
    "    \n",
    "def openConviction_5minCheck(Buying_OD_5, Buying_OTD_5, Buying_ORR_5, Selling_OD_5, Selling_OTD_5, Selling_ORR_5):\n",
    "    # Buying_OD - 1, Buying_OTD - 2, Buying_ORR - 3, Selling_OD - 4, Selling_OTD - 5, Selling_ORR - 6, Others - 7\n",
    "    if Buying_OD_5 == 1:\n",
    "        return 1\n",
    "    elif Buying_OTD_5 == 1:\n",
    "        return 2\n",
    "    elif Selling_OD_5 == 1:\n",
    "        return 4\n",
    "    elif Selling_OTD_5 == 1:\n",
    "        return 5\n",
    "    elif Buying_ORR_5 == 1:\n",
    "        return 3\n",
    "    elif Selling_ORR_5 == 1:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def openConviction_15minCheck(Buying_OD_15, Buying_OTD_15, Buying_ORR_15, Selling_OD_15, Selling_OTD_15, Selling_ORR_15):\n",
    "    # Buying_OD - 1, Buying_OTD - 2, Buying_ORR - 3, Selling_OD - 4, Selling_OTD - 5, Selling_ORR - 6, Others - 7\n",
    "    if Buying_OD_15 == 1:\n",
    "        return 1\n",
    "    elif Buying_OTD_15 == 1:\n",
    "        return 2\n",
    "    elif Selling_OD_15 == 1:\n",
    "        return 4\n",
    "    elif Selling_OTD_15 == 1:\n",
    "        return 5\n",
    "    elif Buying_ORR_15 == 1:\n",
    "        return 3\n",
    "    elif Selling_ORR_15 == 1:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "    \n",
    "def openConviction_30minCheck(StrongUp, ModerateUp, StrongDown, ModerateDown):\n",
    "    # Strong Up - 1, Moderate Up - 2, Strong Down - 3, Moderate Down - 4, Other - 5\n",
    "    if StrongUp == 1:\n",
    "        return 1\n",
    "    elif ModerateUp == 1:\n",
    "        return 2\n",
    "    elif StrongDown == 1:\n",
    "        return 3\n",
    "    elif ModerateDown == 1:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "def openConviction_5min_2Check(Buying_OD_5_2, Buying_OTD_5_2, Buying_ORR_5_2, Selling_OD_5_2, Selling_OTD_5_2, Selling_ORR_5_2):\n",
    "    if Buying_OD_5_2 == 1:\n",
    "        return 1\n",
    "    elif Buying_OTD_5_2 == 1:\n",
    "        return 2\n",
    "    elif Selling_OD_5_2 == 1:\n",
    "        return 4\n",
    "    elif Selling_OTD_5_2 == 1:\n",
    "        return 5\n",
    "    elif Buying_ORR_5_2 == 1:\n",
    "        return 3\n",
    "    elif Selling_ORR_5_2 == 1:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa818f5-95e0-45c1-9066-df6bc20ab8de",
     "showTitle": true,
     "title": "Reading the input tables"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+------+----------+-------------------+-------------------+------+------+\n",
      "|Stock_Name|   Open|   High|    Low|  Close|Volume|      Date|               Time|          Timestamp|CurTop|CurBot|\n",
      "+----------+-------+-------+-------+-------+------+----------+-------------------+-------------------+------+------+\n",
      "|       ACC| 1347.7| 1350.0| 1343.0| 1350.0|  1416|2017-01-04|1970-01-01 09:15:00|2017-01-04 09:15:00|1350.0|1343.0|\n",
      "|       ACC| 1349.0| 1349.0| 1347.0| 1347.0|   539|2017-01-04|1970-01-01 09:16:00|2017-01-04 09:16:00|1350.0|1343.0|\n",
      "|       ACC| 1345.1|1345.55| 1344.3| 1344.7|   457|2017-01-04|1970-01-01 09:17:00|2017-01-04 09:17:00|1350.0|1343.0|\n",
      "|       ACC| 1345.9| 1347.2|1345.05|1346.55|   948|2017-01-04|1970-01-01 09:18:00|2017-01-04 09:18:00|1350.0|1343.0|\n",
      "|       ACC|1346.55| 1347.1|1345.55| 1346.1|   746|2017-01-04|1970-01-01 09:19:00|2017-01-04 09:19:00|1350.0|1343.0|\n",
      "+----------+-------+-------+-------+-------+------+----------+-------------------+-------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(url=jdbcUrl,table=jdbcTable,properties = connectionProperties)\n",
    "column_list = [\"Stock_Name\",\"Date\"]\n",
    "\n",
    "w = Window.partitionBy(*column_list).orderBy(\"Timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"CurTop\", F.max(\"High\").over(w))\n",
    "df = df.withColumn(\"CurBot\", F.min(\"Low\").over(w))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8100f10e-853c-4474-acf6-6057b97686b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+------+------+----------+-------------------+\n",
      "|Stock_Name|   Open|   High|    Low| Close|Volume|      Date|          Timestamp|\n",
      "+----------+-------+-------+-------+------+------+----------+-------------------+\n",
      "|       ACC|1339.95|1357.45| 1339.0|1354.0|193669|2017-01-02|2017-01-02 09:15:00|\n",
      "|       ACC| 1355.0|1355.25|1336.65|1344.0|166583|2017-01-03|2017-01-03 09:15:00|\n",
      "|       ACC| 1347.7| 1351.0|1315.15|1318.8|204497|2017-01-04|2017-01-04 09:15:00|\n",
      "|       ACC| 1330.5| 1339.0| 1320.1|1334.9|362725|2017-01-05|2017-01-05 09:15:00|\n",
      "|       ACC|1341.75| 1349.1| 1330.0|1331.1|247838|2017-01-06|2017-01-06 09:15:00|\n",
      "+----------+-------+-------+-------+------+------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_daily = spark.read.jdbc(url=jdbcUrl,table=jdbcTable_daily,properties = connectionProperties)\n",
    "df_daily = df_daily.orderBy(\"Stock_Name\", \"Date\", ascending=[True, True])\n",
    "df_daily.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61800be0-5fcb-4481-aa3e-e13b04843224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+------+----------+-------------------+\n",
      "|Stock_Name|   Open|   High|    Low|  Close|Volume|      Date|          Timestamp|\n",
      "+----------+-------+-------+-------+-------+------+----------+-------------------+\n",
      "|       ACC|1339.95|1357.45| 1339.0|1349.25| 31878|2017-01-02|2017-01-02 09:15:00|\n",
      "|       ACC| 1349.2| 1352.0| 1346.0| 1350.5|  7887|2017-01-02|2017-01-02 09:30:00|\n",
      "|       ACC| 1350.9| 1351.0| 1348.0| 1350.0|  5005|2017-01-02|2017-01-02 09:45:00|\n",
      "|       ACC| 1350.0| 1352.6|1349.15| 1350.0|  3445|2017-01-02|2017-01-02 10:00:00|\n",
      "|       ACC| 1350.0|1352.85| 1349.1| 1351.1|  7639|2017-01-02|2017-01-02 10:15:00|\n",
      "+----------+-------+-------+-------+-------+------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_15min = spark.read.jdbc(url=jdbcUrl,table=jdbcTable_15,properties = connectionProperties)\n",
    "df_15min = df_15min.orderBy(\"Stock_Name\", \"Date\", \"Timestamp\", ascending=[True, True, True])\n",
    "df_15min.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb418f2-af59-4251-bcf5-0f32289564bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_5min = spark.read.jdbc(url=jdbcUrl,table=jdbcTable_5,properties = connectionProperties)\n",
    "df_5min = df_5min.orderBy(\"Stock_Name\", \"Date\", \"Timestamp\", ascending=[True, True, True])\n",
    "#df_5min.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d4b5d0-4bd5-4d44-abac-8cfacb72cb72",
     "showTitle": true,
     "title": "Reading the output table to check if there are already available data"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ref_levels = spark.read.jdbc(url=jdbcUrl,table=jdbcTable_refLevels,properties = connectionProperties)\n",
    "df_ref_levels_to_be_appended = df_ref_levels.filter(df_ref_levels.Stock_Name == \"NA\").sort(df_ref_levels.Date)\n",
    "\n",
    "# show last 10 items\n",
    "df_ref_levels.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7582dadf-7e02-44dd-8ebe-3fb450486519",
     "showTitle": true,
     "title": "Creating a basic output table"
    }
   },
   "outputs": [],
   "source": [
    "df_output = df_daily\n",
    "df_output = df_output.drop(*['Volume', 'Timestamp'])\n",
    "#df_output.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7798212-b7a0-4d81-9e55-f266072732f7",
     "showTitle": true,
     "title": "Calculating Den"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+------+----------+-------------+------------+-----------+----------+----+--------+------+------+----------+-----+\n",
      "|Stock_Name|   Open|   High|    Low| Close|      Date|Previous_High|Previous_Low|High_of_Two|Low_of_Two| Den|DayRange| baseY|  maxY|todayRange|  ATR|\n",
      "+----------+-------+-------+-------+------+----------+-------------+------------+-----------+----------+----+--------+------+------+----------+-----+\n",
      "|       ACC|1339.95|1357.45| 1339.0|1354.0|2017-01-02|         NULL|        NULL|    1357.45|    1339.0|0.25|   18.45|1339.0|1357.5|      74.0|18.45|\n",
      "|       ACC| 1355.0|1355.25|1336.65|1344.0|2017-01-03|      1357.45|      1339.0|    1357.45|   1336.65| 0.3|    18.6|1336.5|1355.4|      63.0|18.53|\n",
      "|       ACC| 1347.7| 1351.0|1315.15|1318.8|2017-01-04|      1355.25|     1336.65|    1355.25|   1315.15| 0.6|   35.85|1314.6|1351.2|      61.0| 24.3|\n",
      "|       ACC| 1330.5| 1339.0| 1320.1|1334.9|2017-01-05|       1351.0|     1315.15|     1351.0|   1315.15| 0.5|    18.9|1320.0|1339.0|      38.0|22.95|\n",
      "|       ACC|1341.75| 1349.1| 1330.0|1331.1|2017-01-06|       1339.0|      1320.1|     1349.1|    1320.1| 0.4|    19.1|1330.0|1349.2|      48.0|22.18|\n",
      "+----------+-------+-------+-------+------+----------+-------------+------------+-----------+----------+----+--------+------+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"Stock_Name\").orderBy(\"Date\")\n",
    "df_output = df_output.withColumn(\"Previous_High\", F.lag(\"High\").over(w)).withColumn(\"Previous_Low\", F.lag(\"Low\").over(w))\n",
    "df_output = df_output.withColumn(\"High_of_Two\",F.greatest(*[\"High\",\"Previous_High\"])).withColumn(\"Low_of_Two\",F.least(*[\"Low\",\"Previous_Low\"]))\n",
    "df_output = df_output.withColumn(\"Den\", F.round(F.greatest(F.floor(F.round(df_output.High_of_Two - df_output.Low_of_Two, 2) * 0.3) * 0.05, F.lit(0.05)), 2))\n",
    "df_output = df_output.withColumn(\"DayRange\", F.round(df_output.High - df_output.Low, 2))\\\n",
    "            .withColumn(\"baseY\", F.round(F.floor(df_output.Low / df_output.Den) * df_output.Den, 2))\\\n",
    "            .withColumn(\"maxY\", F.round(F.ceil(df_output.High / df_output.Den) * df_output.Den, 2))\n",
    "df_output = df_output.withColumn(\"todayRange\", F.round((df_output.maxY - df_output.baseY) / df_output.Den, 0))\n",
    "w = Window.partitionBy(\"Stock_Name\").orderBy(\"Date\").rowsBetween(-9, 0)\n",
    "df_output = df_output.withColumn(\"ATR\", F.round(F.mean(df_output.DayRange).over(w), 2))\n",
    "df_output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "655c7c9d-b055-4c1d-b800-20c43be63c26",
     "showTitle": true,
     "title": "Validations"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#baseY <= Low\n",
    "print(df_output.filter(df_output.baseY > df_output.Low).count() == 0)\n",
    "\n",
    "#maxY >= High\n",
    "print(df_output.filter(df_output.maxY < df_output.High).count() == 0)\n",
    "\n",
    "#baseY + todayRange * Den == maxY\n",
    "print(df_output.filter(df_output.maxY != F.round(df_output.baseY + (df_output.todayRange * df_output.Den), 2)).count() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa67e39c-352c-4b63-8349-32e251772d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-02\n",
      "Garbage collector: collected 503 objects.\n",
      "2017-01-03\n",
      "Garbage collector: collected 410 objects.\n",
      "2017-01-04\n",
      "Garbage collector: collected 431 objects.\n",
      "2017-01-05\n",
      "Garbage collector: collected 431 objects.\n",
      "2017-01-06\n",
      "Garbage collector: collected 431 objects.\n",
      "2017-01-09\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-10\n",
      "Garbage collector: collected 431 objects.\n",
      "2017-01-11\n",
      "Garbage collector: collected 431 objects.\n",
      "2017-01-12\n",
      "Garbage collector: collected 431 objects.\n",
      "2017-01-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"c:\\Program Files\\Python38\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 647\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Write to the reference levels table and flush the append table'''\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_ref_levels_to_be_appended\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m (endDate \u001b[38;5;241m-\u001b[39m startDate)\u001b[38;5;241m.\u001b[39mdays \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \n\u001b[1;32m--> 647\u001b[0m     \u001b[43mdf_ref_levels_to_be_appended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbcUrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbcTable_refLevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconnectionProperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[0;32m    648\u001b[0m     df_ref_levels_to_be_appended \u001b[38;5;241m=\u001b[39m df_ref_levels_to_be_appended\u001b[38;5;241m.\u001b[39mfilter(df_ref_levels_to_be_appended\u001b[38;5;241m.\u001b[39mStock_Name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    650\u001b[0m collected \u001b[38;5;241m=\u001b[39m gc\u001b[38;5;241m.\u001b[39mcollect()        \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\sql\\readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Retrieving the stock name and date from daily table and check it in the reference table\n",
    "\n",
    "df_daily = df_daily.filter(df_daily.Stock_Name == stock)\n",
    "\n",
    "startDate = datetime.datetime(2017,1,1)\n",
    "endDate = datetime.datetime(2024,2,29)\n",
    "date_value_prev = datetime.datetime(2017,1,1)\n",
    "\n",
    "for i in range((endDate - startDate).days):\n",
    "    \n",
    "    symbol = stock\n",
    "    startDate += datetime.timedelta(days=1)\n",
    "    date_value = startDate.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    try:\n",
    "        dummy_val = df_daily.filter((df_daily.Stock_Name == symbol) & (df_daily.Date == date_value)).collect()[0]\n",
    "    except:\n",
    "        continue\n",
    "    prev_day_filter = 0\n",
    "    \n",
    "    print(date_value)\n",
    "    #print(date_value_prev)\n",
    "    \n",
    "    #Continue if the stock_date is already present in reference table\n",
    "    if df_ref_levels.filter((df_ref_levels.Stock_Name == symbol) & (df_ref_levels.Date == date_value)).count() != 0:\n",
    "        print(\"Data already present for this date! Skipping!!\")\n",
    "        date_value_prev = date_value\n",
    "        continue\n",
    "    \n",
    "    # Retrieve the previous data reference levels\n",
    "    if i != 0:\n",
    "        try:            \n",
    "            prev_day_filter = df_ref_levels.filter((df_ref_levels.Stock_Name == symbol) & (df_ref_levels.Date == date_value_prev)).collect()[0]\n",
    "        except:\n",
    "            #prev_day_filter = df_ref_levels.collect()[0]\n",
    "            df_prevday_row = spark.createDataFrame([\n",
    "            Row(Stock_Name=\"\", Date=\"\", Den=0.0, \n",
    "                VAH=0.0, VAL=0.0, POC=0.0,\n",
    "                IBH=0.0 , IBL=0.0, IBType=0.0, OpenLocation=0,\n",
    "                OpenConviction_5=0, OpenConviction_15=0,\n",
    "                OpenConviction_30=0, OpenConviction_5_2=0,\n",
    "                First5_Open=0.0, First5_High=0.0, First5_Low=0.0,First5_Close=0.0,\n",
    "                First15_Open=0.0, First15_High=0.0, First15_Low=0.0, First15_Close=0.0, \n",
    "                First30_Open=0.0, First30_High=0.0 ,First30_Low=0.0 , First30_Close=0.0,\n",
    "                Second15_Open=0.0 ,Second15_High=0.0, Second15_Low=0.0 ,Second15_Close=0.0 ,\n",
    "                Second30_Open=0.0, Second30_High=0.0 ,Second30_Low=0.0 ,Second30_Close=0.0 ,\n",
    "                SP_Present=0,\n",
    "                Extreme_Buy_Present=0, Extreme_Buy_Count=0,Extreme_Short_Present=0, Extreme_Short_Count=0,Extreme_Present=0, Extreme_Count=0,\n",
    "                RE_Present=0, TPO_Buy_Count=0, TPO_Short_Count=0, TPO_Count=0, \n",
    "                Value_Shift=0, Market_Sentiment=0, DayRange=0.0,\n",
    "                ATR=0.0) ])\n",
    "            prev_day_filter = df_prevday_row.collect()[0]\n",
    "    \n",
    "    # Filter for the current day\n",
    "    day_filter = df_output.filter((df_output.Stock_Name == symbol) & (df_output.Date == date_value)).collect()[0]  \n",
    "    todayRange = int(getattr(day_filter, \"todayRange\"))\n",
    "    todayOpen = getattr(day_filter, \"Open\")\n",
    "    yestHigh = getattr(day_filter, \"Previous_High\")\n",
    "    yestLow = getattr(day_filter, \"Previous_Low\")\n",
    "    baseY = getattr(day_filter, \"baseY\")\n",
    "    Den = getattr(day_filter, \"Den\")\n",
    "    ATR = getattr(day_filter, \"ATR\")\n",
    "\n",
    "    '''Calculating the Market Profile - TPO Prints'''\n",
    "    data_list = [(todayRange,)]\n",
    "    j_array = spark.createDataFrame(data_list,StructType([ StructField(\"myInt\", IntegerType(), True)]))\n",
    "    j_array = j_array.withColumn(\"baseY\", F.lit(baseY)).withColumn(\"Den\", F.lit(Den))\n",
    "    j_array = j_array.withColumn(\"myArr\", rep_(\"myInt\", \"baseY\", \"Den\"))\n",
    "    \n",
    "    # Getting the high and low for each of the 30min period    \n",
    "    df_15min_temp = df_15min.filter((df_15min.Stock_Name == symbol) & (df_15min.Date == date_value)).sort(df_15min.Timestamp.asc()).select(df_15min.High, df_15min.Low, df_15min.Open, df_15min.Close, df_15min.Timestamp)\n",
    "    \n",
    "    #print(\"df_15min_count: \" + str(df_15min_temp.count()))\n",
    "    if df_15min_temp.count() == 25:\n",
    "        a_period = df_15min_temp.collect()[0:2]\n",
    "        a_high = [ ele.__getattr__(\"High\") for ele in a_period]\n",
    "        a_high.sort(reverse=True)\n",
    "        a_high = list(map(a_high.__getitem__, [0]))[0]\n",
    "        a_low = [ ele.__getattr__(\"Low\") for ele in a_period]\n",
    "        a_low.sort()\n",
    "        a_low = list(map(a_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        b_period = df_15min_temp.collect()[2:4]\n",
    "        b_high = [ ele.__getattr__(\"High\") for ele in b_period]\n",
    "        b_high.sort(reverse=True)\n",
    "        b_high = list(map(b_high.__getitem__, [0]))[0]\n",
    "        b_low = [ ele.__getattr__(\"Low\") for ele in b_period]\n",
    "        b_low.sort()\n",
    "        b_low = list(map(b_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        c_period = df_15min_temp.collect()[4:6]\n",
    "        c_high = [ ele.__getattr__(\"High\") for ele in c_period]\n",
    "        c_high.sort(reverse=True)\n",
    "        c_high = list(map(c_high.__getitem__, [0]))[0]\n",
    "        c_low = [ ele.__getattr__(\"Low\") for ele in c_period]\n",
    "        c_low.sort()\n",
    "        c_low = list(map(c_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        d_period = df_15min_temp.collect()[6:8]\n",
    "        d_high = [ ele.__getattr__(\"High\") for ele in d_period]\n",
    "        d_high.sort(reverse=True)\n",
    "        d_high = list(map(d_high.__getitem__, [0]))[0]\n",
    "        d_low = [ ele.__getattr__(\"Low\") for ele in d_period]\n",
    "        d_low.sort()\n",
    "        d_low = list(map(d_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        e_period = df_15min_temp.collect()[8:10]\n",
    "        e_high = [ ele.__getattr__(\"High\") for ele in e_period]\n",
    "        e_high.sort(reverse=True)\n",
    "        e_high = list(map(e_high.__getitem__, [0]))[0]\n",
    "        e_low = [ ele.__getattr__(\"Low\") for ele in e_period]\n",
    "        e_low.sort()\n",
    "        e_low = list(map(e_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        f_period = df_15min_temp.collect()[10:12]\n",
    "        f_high = [ ele.__getattr__(\"High\") for ele in f_period]\n",
    "        f_high.sort(reverse=True)\n",
    "        f_high = list(map(f_high.__getitem__, [0]))[0]\n",
    "        f_low = [ ele.__getattr__(\"Low\") for ele in f_period]\n",
    "        f_low.sort()\n",
    "        f_low = list(map(f_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        g_period = df_15min_temp.collect()[12:14]\n",
    "        g_high = [ ele.__getattr__(\"High\") for ele in g_period]\n",
    "        g_high.sort(reverse=True)\n",
    "        g_high = list(map(g_high.__getitem__, [0]))[0]\n",
    "        g_low = [ ele.__getattr__(\"Low\") for ele in g_period]\n",
    "        g_low.sort()\n",
    "        g_low = list(map(g_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        h_period = df_15min_temp.collect()[14:16]\n",
    "        h_high = [ ele.__getattr__(\"High\") for ele in h_period]\n",
    "        h_high.sort(reverse=True)\n",
    "        h_high = list(map(h_high.__getitem__, [0]))[0]\n",
    "        h_low = [ ele.__getattr__(\"Low\") for ele in h_period]\n",
    "        h_low.sort()\n",
    "        h_low = list(map(h_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        i_period = df_15min_temp.collect()[16:18]\n",
    "        i_high = [ ele.__getattr__(\"High\") for ele in i_period]\n",
    "        i_high.sort(reverse=True)\n",
    "        i_high = list(map(i_high.__getitem__, [0]))[0]\n",
    "        i_low = [ ele.__getattr__(\"Low\") for ele in i_period]\n",
    "        i_low.sort()\n",
    "        i_low = list(map(i_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        j_period = df_15min_temp.collect()[18:20]\n",
    "        j_high = [ ele.__getattr__(\"High\") for ele in j_period]\n",
    "        j_high.sort(reverse=True)\n",
    "        j_high = list(map(j_high.__getitem__, [0]))[0]\n",
    "        j_low = [ ele.__getattr__(\"Low\") for ele in j_period]\n",
    "        j_low.sort()\n",
    "        j_low = list(map(j_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        k_period = df_15min_temp.collect()[20:22]\n",
    "        k_high = [ ele.__getattr__(\"High\") for ele in k_period]\n",
    "        k_high.sort(reverse=True)\n",
    "        k_high = list(map(k_high.__getitem__, [0]))[0]\n",
    "        k_low = [ ele.__getattr__(\"Low\") for ele in k_period]\n",
    "        k_low.sort()\n",
    "        k_low = list(map(k_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        l_period = df_15min_temp.collect()[22:24]\n",
    "        l_high = [ ele.__getattr__(\"High\") for ele in l_period]\n",
    "        l_high.sort(reverse=True)\n",
    "        l_high = list(map(l_high.__getitem__, [0]))[0]\n",
    "        l_low = [ ele.__getattr__(\"Low\") for ele in l_period]\n",
    "        l_low.sort()\n",
    "        l_low = list(map(l_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        m_period = df_15min_temp.collect()[24:25]\n",
    "        m_high = [ ele.__getattr__(\"High\") for ele in m_period]\n",
    "        m_high.sort(reverse=True)\n",
    "        m_high = list(map(m_high.__getitem__, [0]))[0]\n",
    "        m_low = [ ele.__getattr__(\"Low\") for ele in m_period]\n",
    "        m_low.sort()\n",
    "        m_low = list(map(m_low.__getitem__, [0]))[0]   \n",
    "    else:\n",
    "        continue\n",
    "                \n",
    "    # Creating the J bins\n",
    "    j_bin_list = j_array.collect()[0][3]\n",
    "    j_bin_list = [ '%.2f' % elem for elem in j_bin_list ]\n",
    "    j_bin_list = list(map(float, j_bin_list))\n",
    "    j_bins = np.array(j_bin_list)\n",
    "    \n",
    "    # Getting all the elements for each 30min period and putting the elements in j bins\n",
    "    j_idx = list(range(todayRange + 1))\n",
    "    x_j = list(range(todayRange + 1))\n",
    "    \n",
    "    a_index = np.array([a_low, a_high])\n",
    "    b_index = np.array([b_low, b_high])\n",
    "    c_index = np.array([c_low, c_high])\n",
    "    d_index = np.array([d_low, d_high])\n",
    "    e_index = np.array([e_low, e_high])\n",
    "    f_index = np.array([f_low, f_high])\n",
    "    g_index = np.array([g_low, g_high])\n",
    "    h_index = np.array([h_low, h_high])\n",
    "    i_index = np.array([i_low, i_high])\n",
    "    j_index = np.array([j_low, j_high])\n",
    "    k_index = np.array([k_low, k_high])\n",
    "    l_index = np.array([l_low, l_high])\n",
    "    m_index = np.array([m_low, m_high])\n",
    "    \n",
    "    a_index = np.digitize(a_index, j_bins, right=True)\n",
    "    a_index = list(range(a_index[0], a_index[1] + 1))\n",
    "    \n",
    "    b_index = np.digitize(b_index, j_bins, right=True)\n",
    "    b_index = list(range(b_index[0], b_index[1] + 1))\n",
    "    \n",
    "    c_index = np.digitize(c_index, j_bins, right=True)\n",
    "    c_index = list(range(c_index[0], c_index[1] + 1))\n",
    "    \n",
    "    d_index = np.digitize(d_index, j_bins, right=True)\n",
    "    d_index = list(range(d_index[0], d_index[1] + 1))\n",
    "    \n",
    "    e_index = np.digitize(e_index, j_bins, right=True)\n",
    "    e_index = list(range(e_index[0], e_index[1] + 1))\n",
    "    \n",
    "    f_index = np.digitize(f_index, j_bins, right=True)\n",
    "    f_index = list(range(f_index[0], f_index[1] + 1))\n",
    "    \n",
    "    g_index = np.digitize(g_index, j_bins, right=True)\n",
    "    g_index = list(range(g_index[0], g_index[1] + 1))\n",
    "    \n",
    "    h_index = np.digitize(h_index, j_bins, right=True)\n",
    "    h_index = list(range(h_index[0], h_index[1] + 1))\n",
    "    \n",
    "    i_index = np.digitize(i_index, j_bins, right=True)\n",
    "    i_index = list(range(i_index[0], i_index[1] + 1))\n",
    "    \n",
    "    j_index = np.digitize(j_index, j_bins, right=True)\n",
    "    j_index = list(range(j_index[0], j_index[1] + 1))\n",
    "    \n",
    "    k_index = np.digitize(k_index, j_bins, right=True)\n",
    "    k_index = list(range(k_index[0], k_index[1] + 1))\n",
    "    \n",
    "    l_index = np.digitize(l_index, j_bins, right=True)\n",
    "    l_index = list(range(l_index[0], l_index[1] + 1))\n",
    "    \n",
    "    m_index = np.digitize(m_index, j_bins, right=True)\n",
    "    m_index = list(range(m_index[0], m_index[1] + 1))\n",
    "    \n",
    "    # Calculating the sum of elements in each J bins\n",
    "    # Initialize the output array with zeros\n",
    "    x_j = [0] * len(j_idx)\n",
    "    \n",
    "    # Count occurrences in a_index\n",
    "    for index in a_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in b_index\n",
    "    for index in b_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in c_index\n",
    "    for index in c_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in d_index\n",
    "    for index in d_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in e_index\n",
    "    for index in e_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in f_index\n",
    "    for index in f_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in g_index\n",
    "    for index in g_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in h_index\n",
    "    for index in h_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in i_index\n",
    "    for index in i_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in j_index\n",
    "    for index in j_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in k_index\n",
    "    for index in k_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in l_index\n",
    "    for index in l_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in m_index\n",
    "    for index in m_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    x_j = x_j[1:]\n",
    "    #print(\"Output Array:\", x_j)\n",
    "    #print(\"Output Array created!\")\n",
    "    \n",
    "    '''Validation'''\n",
    "    #print(\"Validation: \" + str(len(x_j) == todayRange))\n",
    "    \n",
    "    '''Calculate the VAH, VAL and POC'''\n",
    "    # Calculate the half price index\n",
    "    halfpriceIndex = int(todayRange/2)\n",
    "    \n",
    "    # Calculate sum of lower and upper count\n",
    "    lowerCount = sum(x_j[0:halfpriceIndex])\n",
    "    upperCount = sum(x_j[halfpriceIndex:])\n",
    "    tpoHighorLow = 1 if lowerCount > upperCount else 2 if lowerCount < upperCount else -1\n",
    "    \n",
    "    # Calculate the POC\n",
    "    maxTPOCounter = x_j.count(max(x_j))\n",
    "    maxTPOArray = [i for i in range(len(x_j)) if x_j[i] == max(x_j)]\n",
    "    \n",
    "    if maxTPOCounter == 1:\n",
    "        maxJ = maxTPOArray[0]\n",
    "    else:\n",
    "        maxTPOdistfromMidArray = [abs(x - halfpriceIndex) for x in maxTPOArray]\n",
    "        minDistCount = maxTPOdistfromMidArray.count(min(maxTPOdistfromMidArray))\n",
    "        minDistArray = [maxTPOArray[i] for i in [i for i in range(len(maxTPOdistfromMidArray)) if maxTPOdistfromMidArray[i] == min(maxTPOdistfromMidArray)]]\n",
    "        if minDistCount == 1:\n",
    "            maxJ = minDistArray[0]\n",
    "        else:\n",
    "            maxJ = minDistArray[0] if tpoHighorLow == 1 or tpoHighorLow == -1 else minDistArray[1]\n",
    "    \n",
    "    # Calculate the VAH and VAL\n",
    "    up_idx, down_idx = find_indexes(x_j, maxJ)\n",
    "    vah = round(baseY + (up_idx * Den),2)\n",
    "    val = round(baseY + (down_idx * Den),2)\n",
    "    poc = round(baseY + (maxJ * Den),2)\n",
    "    \n",
    "    #print(f\"VAL: {val}, POC: {poc}, VAH:{vah}, maxJ:{maxJ}, up:{up_idx}, dn:{down_idx}, base:{baseY}, Den:{Den}\")\n",
    "    \n",
    "    '''Calculate IBH, IBL and IBType'''\n",
    "    # First 60mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min60_filter = df_15min_temp.collect()[0:4]\n",
    "        first60minHigh = [ ele.__getattr__(\"High\") for ele in min60_filter]\n",
    "        first60minHigh.sort(reverse=True)\n",
    "        first60minHigh = round(list(map(first60minHigh.__getitem__, [0]))[0], 2)\n",
    "        first60minLow = [ ele.__getattr__(\"Low\") for ele in min60_filter]\n",
    "        first60minLow.sort()\n",
    "        first60minLow = round(list(map(first60minLow.__getitem__, [0]))[0], 2)\n",
    "        first60minOpen = [ ele.__getattr__(\"Open\") for ele in min60_filter]    \n",
    "        first60minOpen = round(list(map(first60minOpen.__getitem__, [0]))[0], 2)\n",
    "        first60minClose = [ ele.__getattr__(\"Close\") for ele in min60_filter]    \n",
    "        first60minClose = round(list(map(first60minClose.__getitem__, [3]))[0], 2)\n",
    "    else:\n",
    "        continue\n",
    "            \n",
    "    IBH = first60minHigh\n",
    "    IBL = first60minLow\n",
    "    IBRange = round(IBH - IBL, 2)\n",
    "    \n",
    "    IBTarget0High = IBH + (IBRange * 0.25)\n",
    "    IBTarget0Low = IBL - (IBRange * 0.25)\n",
    "    IBTarget1High = IBH + (IBRange * 0.5)\n",
    "    IBTarget1Low = IBL - (IBRange * 0.5)\n",
    "    IBTarget15High = IBH + (IBRange * 0.6)\n",
    "    IBTarget15Low = IBL - (IBRange * 0.6)\n",
    "    IBTarget2High = IBH + (IBRange * 0.8)\n",
    "    IBTarget2Low = IBL - (IBRange * 0.8)\n",
    "    IBTarget3High = IBH + (IBRange * 1)\n",
    "    IBTarget3Low = IBL - (IBRange * 1)\n",
    "    IBTarget4High = IBH + (IBRange * 1.2)\n",
    "    IBTarget4Low = IBL - (IBRange * 1.2)\n",
    "    \n",
    "    IBType = IBTypeCheck(IBRange, ATR)\n",
    "    #print(\"IB levels calculated!\")\n",
    "    \n",
    "    '''Calculate First5, First15, First30, Second15, Second30 and Day OHLC'''\n",
    "    dayHigh = round(getattr(day_filter, \"High\"), 2)\n",
    "    dayLow =  round(getattr(day_filter, \"Low\"), 2)\n",
    "    dayOpen = round(getattr(day_filter, \"Open\"), 2)\n",
    "    dayClose = round(getattr(day_filter, \"Close\"), 2)\n",
    "    \n",
    "    df_5min_temp = df_5min.filter((df_5min.Stock_Name == symbol) & (df_5min.Date == date_value)).sort(df_5min.Timestamp.asc())\n",
    "    \n",
    "    # First 5mins\n",
    "    if df_5min_temp.count() == 75:\n",
    "        min5_filter = df_5min_temp.collect()[0]\n",
    "        first5minHigh = round(getattr(min5_filter, \"High\"), 2)\n",
    "        first5minLow = round(getattr(min5_filter, \"Low\"), 2)\n",
    "        first5minOpen = round(getattr(min5_filter, \"Open\"), 2)\n",
    "        first5minClose = round(getattr(min5_filter, \"Close\"), 2)\n",
    "    else:\n",
    "        first5minHigh = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.High).collect()[0].High, 2)\n",
    "        first5minLow = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.Low).collect()[0].Low, 2)\n",
    "        first5minOpen = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.Open).collect()[0].Open, 2)\n",
    "        first5minClose = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.Close).collect()[0].Close, 2)\n",
    "    \n",
    "    # First 15mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min15_filter = df_15min_temp.collect()[0]\n",
    "        first15minHigh = round(getattr(min15_filter, \"High\"), 2)\n",
    "        first15minLow = round(getattr(min15_filter, \"Low\"), 2)\n",
    "        first15minOpen = round(getattr(min15_filter, \"Open\"), 2)\n",
    "        first15minClose = round(getattr(min15_filter, \"Close\"), 2)\n",
    "    else:\n",
    "        continue\n",
    "            \n",
    "    # First 30mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min30_filter = df_15min_temp.collect()[0:2]\n",
    "        first30minHigh = [ ele.__getattr__(\"High\") for ele in min30_filter]\n",
    "        first30minHigh.sort(reverse=True)\n",
    "        first30minHigh = round(list(map(first30minHigh.__getitem__, [0]))[0], 2)\n",
    "        first30minLow = [ ele.__getattr__(\"Low\") for ele in min30_filter]\n",
    "        first30minLow.sort()\n",
    "        first30minLow = round(list(map(first30minLow.__getitem__, [0]))[0], 2)\n",
    "        first30minOpen = [ ele.__getattr__(\"Open\") for ele in min30_filter]\n",
    "        #first30minOpen.sort()\n",
    "        first30minOpen = round(list(map(first30minOpen.__getitem__, [0]))[0], 2)\n",
    "        first30minClose = [ ele.__getattr__(\"Close\") for ele in min30_filter]\n",
    "        #first30minClose.sort()\n",
    "        first30minClose = round(list(map(first30minClose.__getitem__, [1]))[0], 2)\n",
    "    else:\n",
    "        continue\n",
    "            \n",
    "    # Second 15mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min15_2_filter = df_15min_temp.collect()[1]\n",
    "        second15minHigh = round(getattr(min15_2_filter, \"High\"), 2)\n",
    "        second15minLow = round(getattr(min15_2_filter, \"Low\"), 2)\n",
    "        second15minOpen = round(getattr(min15_2_filter, \"Open\"), 2)\n",
    "        second15minClose = round(getattr(min15_2_filter, \"Close\"), 2)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    # Second 30mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min30_2_filter = df_15min_temp.collect()[2:4]\n",
    "        second30minHigh = [ ele.__getattr__(\"High\") for ele in min30_2_filter]\n",
    "        second30minHigh.sort(reverse=True)\n",
    "        second30minHigh = round(list(map(second30minHigh.__getitem__, [0]))[0], 2)\n",
    "        second30minLow = [ ele.__getattr__(\"Low\") for ele in min30_2_filter]\n",
    "        second30minLow.sort()\n",
    "        second30minLow = round(list(map(second30minLow.__getitem__, [0]))[0], 2)\n",
    "        second30minOpen = [ ele.__getattr__(\"Open\") for ele in min30_2_filter]\n",
    "        #second30minOpen.sort()\n",
    "        second30minOpen = round(list(map(second30minOpen.__getitem__, [0]))[0], 2)\n",
    "        second30minClose = [ ele.__getattr__(\"Close\") for ele in min30_2_filter]\n",
    "        #second30minClose.sort()\n",
    "        second30minClose = round(list(map(second30minClose.__getitem__, [1]))[0], 2)\n",
    "    else:\n",
    "        continue        \n",
    "    \n",
    "    '''Calculate the Extremes, RE, Value Shift and Market Sentiment'''\n",
    "    # Extremes\n",
    "    buyExtremeCount = calculateExtremes(x_j)\n",
    "    shortExtremeCount = calculateExtremes(list(reversed(x_j)))\n",
    "    netExtremeCount = buyExtremeCount - shortExtremeCount\n",
    "    \n",
    "    buyExtremePresent = 1 if buyExtremeCount >= 2 else 0\n",
    "    shortExtremePresent = 1 if shortExtremeCount >= 2 else 0  \n",
    "    extremePresent = extremePresentCheck(buyExtremePresent, shortExtremePresent, netExtremeCount)\n",
    "    \n",
    "    # Range Extension\n",
    "    buyRangeExtension = 1 if dayHigh >= IBTarget1High else 0\n",
    "    shortRangeExtension = 1 if dayLow <= IBTarget1Low else 0\n",
    "    rangeExtension = rangeExtensionCheck(buyRangeExtension, shortRangeExtension)\n",
    "    \n",
    "    # TPO Count\n",
    "    buyTPOCount = sum(x_j[:maxJ])\n",
    "    shortTPOCount = sum(x_j[maxJ + 1:])\n",
    "    tpoCount = tpoCountCheck(buyTPOCount, shortTPOCount)\n",
    "    \n",
    "    # Value Shift\n",
    "    # 1 -> Positive, 2 -> Negative, 3 -> Neutral\n",
    "    vah = round(baseY + (up_idx * Den),2)\n",
    "    val = round(baseY + (down_idx * Den),2)\n",
    "    poc = round(baseY + (maxJ * Den),2)\n",
    "    \n",
    "    if i != 0:\n",
    "        vah_prev = getattr(prev_day_filter, \"VAH\")\n",
    "        val_prev = getattr(prev_day_filter, \"VAL\")\n",
    "        poc_prev = getattr(prev_day_filter, \"POC\")\n",
    "    else:\n",
    "        vah_prev = 999999\n",
    "        val_prev = 0\n",
    "        poc_prev = 999999\n",
    "        \n",
    "    valueShift = 1 if (vah >= vah_prev * 1.005 or val >= val_prev * 1.005) and poc > poc_prev else 2 if (val <= val_prev * 0.995 or vah <= vah_prev * 0.995) and poc < poc_prev else 3\n",
    "    \n",
    "    # Market Sentiment\n",
    "    marketSentiment = marketSentimentCheck(extremePresent, rangeExtension, tpoCount, valueShift)\n",
    "    #print(\"MP levels calculated!\")\n",
    "    \n",
    "    '''Open Location and Open Conviction'''\n",
    "    openLocation = openLocationCheck(todayOpen, yestHigh, yestLow, vah_prev, val_prev)\n",
    "    \n",
    "    # Open Conviction - 5mins\n",
    "    Buying_OD_5 = first5minOpen == first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen\n",
    "    Buying_OTD_5 = first5minOpen != first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen and first5minLow > yestHigh * 0.999\n",
    "    Buying_ORR_5 = first5minClose <= ((first5minHigh - first5minLow) * 0.3) + first5minLow and first5minOpen > first5minClose\n",
    "    Selling_OD_5 = first5minOpen == first5minHigh and first5minClose <= (first5minHigh - ((first5minHigh - first5minLow) * 0.7)) and first5minClose < first5minOpen \n",
    "    Selling_OTD_5 = first5minOpen != first5minHigh and first5minClose <= (first5minHigh - ((first5minHigh - first5minLow) * 0.7)) and first5minClose < first5minOpen and first5minHigh < yestLow * 1.001\n",
    "    Selling_ORR_5 = first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minOpen < first5minClose\n",
    "    openConviction_5 = openConviction_5minCheck(Buying_OD_5, Buying_OTD_5, Buying_ORR_5, Selling_OD_5, Selling_OTD_5, Selling_ORR_5)\n",
    "    \n",
    "    # Open Conviction - 15mins\n",
    "    Buying_OD_15 = first15minOpen == first15minLow and first15minClose >= ((first15minHigh - first15minLow) * 0.7) + first15minLow and first15minClose > first15minOpen\n",
    "    Buying_OTD_15 = first15minOpen != first15minLow and first15minClose >= ((first15minHigh - first15minLow) * 0.7) + first15minLow and first15minClose > first15minOpen and first15minLow > yestHigh\n",
    "    Buying_ORR_15 = first15minOpen > first15minClose or (first15minClose <= ((first15minHigh - first15minLow) * 0.7) + first15minLow and first15minClose > first15minOpen) or first15minLow <= yestHigh\n",
    "    Selling_OD_15 = first15minOpen == first15minHigh and first15minClose <= (first15minHigh - ((first15minHigh - first15minLow) * 0.7)) and first15minClose < first15minOpen \n",
    "    Selling_OTD_15 = first15minOpen != first15minHigh and first15minClose <= (first15minHigh - ((first15minHigh - first15minLow) * 0.7)) and first15minClose < first15minOpen and first15minHigh < yestLow\n",
    "    Selling_ORR_15 = first15minOpen < first15minClose or (first15minClose >= (first15minHigh - ((first15minHigh - first15minLow) * 0.7)) and first15minClose < first15minOpen) or first15minHigh >= yestLow\n",
    "    openConviction_15 = openConviction_15minCheck(Buying_OD_15, Buying_OTD_15, Buying_ORR_15, Selling_OD_15, Selling_OTD_15, Selling_ORR_15)\n",
    "    \n",
    "    # Open Conviction - 30mins\n",
    "    Uncertain = (first15minClose > first15minOpen and second15minOpen > second15minClose) or (first15minClose < first15minOpen and second15minOpen < second15minClose)\n",
    "    StrongUp = (first30minOpen == first30minLow and first30minClose >= ((first30minHigh - first30minLow) * 0.7) + first30minLow and first30minClose > first30minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow) or (first30minOpen == first30minLow and first15minClose > first15minOpen and second15minClose > second15minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow)\n",
    "    ModerateUp = (first30minOpen <= ((first30minHigh - first30minLow) * 0.2) + first30minLow and first30minClose >= ((first30minHigh - first30minLow) * 0.7) + first30minLow and first30minClose > first30minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow) or (first30minOpen <= ((first30minHigh - first30minLow) * 0.2) + first30minLow and first15minClose > first15minOpen and second15minClose > second15minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow)\n",
    "    StrongDown = (first30minOpen == first30minHigh and first30minClose <= first30minHigh - ((first30minHigh - first30minLow) * 0.7) and first30minClose < first30minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5)) or (first30minOpen == first30minHigh and first15minClose < first15minOpen and second15minClose < second15minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5))\n",
    "    ModerateDown = (first30minOpen >= first30minHigh - ((first30minHigh - first30minLow) * 0.2) and first30minClose <= first30minHigh - ((first30minHigh - first30minLow) * 0.7) and first30minClose < first30minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5)) or (first30minOpen >= first30minHigh - ((first30minHigh - first30minLow) * 0.2) and first15minClose < first15minOpen and second15minClose < second15minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5))\n",
    "    openConviction_30 = openConviction_30minCheck(StrongUp, ModerateUp, StrongDown, ModerateDown)\n",
    "    \n",
    "    # Open Conviction - 5mins_2\n",
    "    Buying_OD_5_2 = first5minOpen == first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen\n",
    "    Buying_OTD_5_2 = first5minOpen != first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen and first5minLow > vah_prev * 0.999\n",
    "    Buying_ORR_5_2 = first5minClose <= ((first5minHigh - first5minLow) * 0.3) + first5minLow and first5minOpen > first5minClose\n",
    "    Selling_OD_5_2 = first5minOpen == first5minHigh and first5minClose <= first5minHigh - ((first5minHigh - first5minLow) * 0.7) and first5minClose < first5minOpen \n",
    "    Selling_OTD_5_2 = first5minOpen != first5minHigh and first5minClose <= first5minHigh - ((first5minHigh - first5minLow) * 0.7) and first5minClose < first5minOpen and first5minHigh < val_prev * 1.001\n",
    "    Selling_ORR_5_2 = first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minOpen < first5minClose\n",
    "    openConviction_5_2 = openConviction_5min_2Check(Buying_OD_5_2, Buying_OTD_5_2, Buying_ORR_5_2, Selling_OD_5_2, Selling_OTD_5_2, Selling_ORR_5_2)\n",
    "    #print(\"Open convictions calculated!\")\n",
    "    \n",
    "    '''Insert into reference table'''\n",
    "    df_output_row = spark.createDataFrame([\n",
    "        Row(Stock_Name=symbol, Date=date_value, Den=Den, \n",
    "            VAH=vah, VAL=val, POC=poc,\n",
    "            IBH=IBH , IBL=IBL, IBType=IBType, OpenLocation=openLocation,\n",
    "            OpenConviction_5=openConviction_5, OpenConviction_15=openConviction_15,\n",
    "            OpenConviction_30=openConviction_30, OpenConviction_5_2=openConviction_5_2,\n",
    "            First5_Open=first5minOpen, First5_High=first5minHigh, First5_Low=first5minLow,First5_Close=first5minClose,\n",
    "            First15_Open=first15minOpen, First15_High=first15minHigh, First15_Low=first15minLow, First15_Close=first15minClose, \n",
    "            First30_Open=first30minOpen, First30_High=first30minHigh ,First30_Low=first30minLow , First30_Close=first30minClose,\n",
    "            Second15_Open=second15minOpen ,Second15_High=second15minHigh, Second15_Low=second15minLow ,Second15_Close=second15minClose ,\n",
    "            Second30_Open=second30minOpen, Second30_High=second30minHigh ,Second30_Low=second30minLow ,Second30_Close=second30minClose ,\n",
    "            SP_Present=0,\n",
    "            Extreme_Buy_Present=buyExtremePresent, Extreme_Buy_Count=buyExtremeCount,Extreme_Short_Present=shortExtremePresent, Extreme_Short_Count=shortExtremeCount,Extreme_Present=extremePresent, Extreme_Count=netExtremeCount,\n",
    "            RE_Present=rangeExtension, TPO_Buy_Count=buyTPOCount, TPO_Short_Count=shortTPOCount, TPO_Count=tpoCount, \n",
    "            Value_Shift=valueShift, Market_Sentiment=marketSentiment, DayRange=todayRange,\n",
    "            ATR=ATR)  ])    \n",
    "    # df_ref_levels = df_output_row\n",
    "    date_value_prev = date_value\n",
    "    df_ref_levels_to_be_appended = unionAll([df_ref_levels_to_be_appended, df_output_row])\n",
    "    \n",
    "    del x_j\n",
    "    del j_bin_list\n",
    "    del j_bins\n",
    "    del j_idx\n",
    "    del data_list\n",
    "    del a_period\n",
    "    del b_period\n",
    "    del c_period\n",
    "    del d_period\n",
    "    del e_period\n",
    "    del f_period\n",
    "    del g_period\n",
    "    del h_period\n",
    "    del i_period\n",
    "    del j_period\n",
    "    del k_period\n",
    "    del l_period\n",
    "    del m_period\n",
    "    del a_high\n",
    "    del b_high\n",
    "    del c_high\n",
    "    del d_high\n",
    "    del e_high\n",
    "    del f_high\n",
    "    del g_high\n",
    "    del h_high\n",
    "    del i_high\n",
    "    del j_high\n",
    "    del k_high\n",
    "    del l_high\n",
    "    del m_high\n",
    "    del a_low\n",
    "    del b_low\n",
    "    del c_low\n",
    "    del d_low\n",
    "    del e_low\n",
    "    del f_low\n",
    "    del g_low\n",
    "    del h_low\n",
    "    del i_low\n",
    "    del j_low\n",
    "    del k_low\n",
    "    del l_low\n",
    "    del m_low\n",
    "    del a_index\n",
    "    del b_index\n",
    "    del c_index\n",
    "    del d_index\n",
    "    del e_index\n",
    "    del f_index\n",
    "    del g_index\n",
    "    del h_index\n",
    "    del i_index\n",
    "    del j_index\n",
    "    del k_index\n",
    "    del l_index\n",
    "    del m_index\n",
    "    del df_output_row\n",
    "    \n",
    "\n",
    "    # x_j.clear()\n",
    "    # j_bin_list.clear()\n",
    "    # j_idx.clear()\n",
    "    # data_list.clear()\n",
    "    # a_period.clear()\n",
    "    # b_period.clear()\n",
    "    # c_period.clear()\n",
    "    # d_period.clear()\n",
    "    # e_period.clear()\n",
    "    # f_period.clear()\n",
    "    # g_period.clear()\n",
    "    # h_period.clear()\n",
    "    # i_period.clear()\n",
    "    # j_period.clear()\n",
    "    # k_period.clear()\n",
    "    # l_period.clear()\n",
    "    # m_period.clear()\n",
    "        \n",
    "    '''Write to the reference levels table and flush the append table'''\n",
    "    if df_ref_levels_to_be_appended.count() == 10 or i == (endDate - startDate).days - 1:  \n",
    "        df_ref_levels_to_be_appended.write.jdbc(url=jdbcUrl,table=jdbcTable_refLevels,properties = connectionProperties,mode=\"append\")    \n",
    "        df_ref_levels_to_be_appended = df_ref_levels_to_be_appended.filter(df_ref_levels_to_be_appended.Stock_Name == \"NA\")\n",
    "    \n",
    "    collected = gc.collect()        \n",
    " \n",
    "    print(\"Garbage collector: collected %d objects.\" % (collected))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateValueArea(day_filter, df_15min_temp, df_5min_temp, df_ref_levels_to_be_appended):\n",
    "    todayRange = int(getattr(day_filter, \"todayRange\"))\n",
    "    todayOpen = getattr(day_filter, \"Open\")\n",
    "    yestHigh = getattr(day_filter, \"Previous_High\")\n",
    "    yestLow = getattr(day_filter, \"Previous_Low\")\n",
    "    baseY = getattr(day_filter, \"baseY\")\n",
    "    Den = getattr(day_filter, \"Den\")\n",
    "    ATR = getattr(day_filter, \"ATR\")\n",
    "\n",
    "    '''Calculating the Market Profile - TPO Prints'''\n",
    "    data_list = [(todayRange,)]\n",
    "    j_array = spark.createDataFrame(data_list,StructType([ StructField(\"myInt\", IntegerType(), True)]))\n",
    "    j_array = j_array.withColumn(\"baseY\", F.lit(baseY)).withColumn(\"Den\", F.lit(Den))\n",
    "    j_array = j_array.withColumn(\"myArr\", rep_(\"myInt\", \"baseY\", \"Den\"))\n",
    "    \n",
    "    \n",
    "    #print(\"df_15min_count: \" + str(df_15min_temp.count()))\n",
    "    if df_15min_temp.count() == 25:\n",
    "        a_period = df_15min_temp.collect()[0:2]\n",
    "        a_high = [ ele.__getattr__(\"High\") for ele in a_period]\n",
    "        a_high.sort(reverse=True)\n",
    "        a_high = list(map(a_high.__getitem__, [0]))[0]\n",
    "        a_low = [ ele.__getattr__(\"Low\") for ele in a_period]\n",
    "        a_low.sort()\n",
    "        a_low = list(map(a_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        b_period = df_15min_temp.collect()[2:4]\n",
    "        b_high = [ ele.__getattr__(\"High\") for ele in b_period]\n",
    "        b_high.sort(reverse=True)\n",
    "        b_high = list(map(b_high.__getitem__, [0]))[0]\n",
    "        b_low = [ ele.__getattr__(\"Low\") for ele in b_period]\n",
    "        b_low.sort()\n",
    "        b_low = list(map(b_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        c_period = df_15min_temp.collect()[4:6]\n",
    "        c_high = [ ele.__getattr__(\"High\") for ele in c_period]\n",
    "        c_high.sort(reverse=True)\n",
    "        c_high = list(map(c_high.__getitem__, [0]))[0]\n",
    "        c_low = [ ele.__getattr__(\"Low\") for ele in c_period]\n",
    "        c_low.sort()\n",
    "        c_low = list(map(c_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        d_period = df_15min_temp.collect()[6:8]\n",
    "        d_high = [ ele.__getattr__(\"High\") for ele in d_period]\n",
    "        d_high.sort(reverse=True)\n",
    "        d_high = list(map(d_high.__getitem__, [0]))[0]\n",
    "        d_low = [ ele.__getattr__(\"Low\") for ele in d_period]\n",
    "        d_low.sort()\n",
    "        d_low = list(map(d_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        e_period = df_15min_temp.collect()[8:10]\n",
    "        e_high = [ ele.__getattr__(\"High\") for ele in e_period]\n",
    "        e_high.sort(reverse=True)\n",
    "        e_high = list(map(e_high.__getitem__, [0]))[0]\n",
    "        e_low = [ ele.__getattr__(\"Low\") for ele in e_period]\n",
    "        e_low.sort()\n",
    "        e_low = list(map(e_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        f_period = df_15min_temp.collect()[10:12]\n",
    "        f_high = [ ele.__getattr__(\"High\") for ele in f_period]\n",
    "        f_high.sort(reverse=True)\n",
    "        f_high = list(map(f_high.__getitem__, [0]))[0]\n",
    "        f_low = [ ele.__getattr__(\"Low\") for ele in f_period]\n",
    "        f_low.sort()\n",
    "        f_low = list(map(f_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        g_period = df_15min_temp.collect()[12:14]\n",
    "        g_high = [ ele.__getattr__(\"High\") for ele in g_period]\n",
    "        g_high.sort(reverse=True)\n",
    "        g_high = list(map(g_high.__getitem__, [0]))[0]\n",
    "        g_low = [ ele.__getattr__(\"Low\") for ele in g_period]\n",
    "        g_low.sort()\n",
    "        g_low = list(map(g_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        h_period = df_15min_temp.collect()[14:16]\n",
    "        h_high = [ ele.__getattr__(\"High\") for ele in h_period]\n",
    "        h_high.sort(reverse=True)\n",
    "        h_high = list(map(h_high.__getitem__, [0]))[0]\n",
    "        h_low = [ ele.__getattr__(\"Low\") for ele in h_period]\n",
    "        h_low.sort()\n",
    "        h_low = list(map(h_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        i_period = df_15min_temp.collect()[16:18]\n",
    "        i_high = [ ele.__getattr__(\"High\") for ele in i_period]\n",
    "        i_high.sort(reverse=True)\n",
    "        i_high = list(map(i_high.__getitem__, [0]))[0]\n",
    "        i_low = [ ele.__getattr__(\"Low\") for ele in i_period]\n",
    "        i_low.sort()\n",
    "        i_low = list(map(i_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        j_period = df_15min_temp.collect()[18:20]\n",
    "        j_high = [ ele.__getattr__(\"High\") for ele in j_period]\n",
    "        j_high.sort(reverse=True)\n",
    "        j_high = list(map(j_high.__getitem__, [0]))[0]\n",
    "        j_low = [ ele.__getattr__(\"Low\") for ele in j_period]\n",
    "        j_low.sort()\n",
    "        j_low = list(map(j_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        k_period = df_15min_temp.collect()[20:22]\n",
    "        k_high = [ ele.__getattr__(\"High\") for ele in k_period]\n",
    "        k_high.sort(reverse=True)\n",
    "        k_high = list(map(k_high.__getitem__, [0]))[0]\n",
    "        k_low = [ ele.__getattr__(\"Low\") for ele in k_period]\n",
    "        k_low.sort()\n",
    "        k_low = list(map(k_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        l_period = df_15min_temp.collect()[22:24]\n",
    "        l_high = [ ele.__getattr__(\"High\") for ele in l_period]\n",
    "        l_high.sort(reverse=True)\n",
    "        l_high = list(map(l_high.__getitem__, [0]))[0]\n",
    "        l_low = [ ele.__getattr__(\"Low\") for ele in l_period]\n",
    "        l_low.sort()\n",
    "        l_low = list(map(l_low.__getitem__, [0]))[0]\n",
    "    \n",
    "        m_period = df_15min_temp.collect()[24:25]\n",
    "        m_high = [ ele.__getattr__(\"High\") for ele in m_period]\n",
    "        m_high.sort(reverse=True)\n",
    "        m_high = list(map(m_high.__getitem__, [0]))[0]\n",
    "        m_low = [ ele.__getattr__(\"Low\") for ele in m_period]\n",
    "        m_low.sort()\n",
    "        m_low = list(map(m_low.__getitem__, [0]))[0]   \n",
    "    else:\n",
    "        return\n",
    "                \n",
    "    # Creating the J bins\n",
    "    j_bin_list = j_array.collect()[0][3]\n",
    "    j_bin_list = [ '%.2f' % elem for elem in j_bin_list ]\n",
    "    j_bin_list = list(map(float, j_bin_list))\n",
    "    j_bins = np.array(j_bin_list)\n",
    "    \n",
    "    # Getting all the elements for each 30min period and putting the elements in j bins\n",
    "    j_idx = list(range(todayRange + 1))\n",
    "    x_j = list(range(todayRange + 1))\n",
    "    \n",
    "    a_index = np.array([a_low, a_high])\n",
    "    b_index = np.array([b_low, b_high])\n",
    "    c_index = np.array([c_low, c_high])\n",
    "    d_index = np.array([d_low, d_high])\n",
    "    e_index = np.array([e_low, e_high])\n",
    "    f_index = np.array([f_low, f_high])\n",
    "    g_index = np.array([g_low, g_high])\n",
    "    h_index = np.array([h_low, h_high])\n",
    "    i_index = np.array([i_low, i_high])\n",
    "    j_index = np.array([j_low, j_high])\n",
    "    k_index = np.array([k_low, k_high])\n",
    "    l_index = np.array([l_low, l_high])\n",
    "    m_index = np.array([m_low, m_high])\n",
    "    \n",
    "    a_index = np.digitize(a_index, j_bins, right=True)\n",
    "    a_index = list(range(a_index[0], a_index[1] + 1))\n",
    "    \n",
    "    b_index = np.digitize(b_index, j_bins, right=True)\n",
    "    b_index = list(range(b_index[0], b_index[1] + 1))\n",
    "    \n",
    "    c_index = np.digitize(c_index, j_bins, right=True)\n",
    "    c_index = list(range(c_index[0], c_index[1] + 1))\n",
    "    \n",
    "    d_index = np.digitize(d_index, j_bins, right=True)\n",
    "    d_index = list(range(d_index[0], d_index[1] + 1))\n",
    "    \n",
    "    e_index = np.digitize(e_index, j_bins, right=True)\n",
    "    e_index = list(range(e_index[0], e_index[1] + 1))\n",
    "    \n",
    "    f_index = np.digitize(f_index, j_bins, right=True)\n",
    "    f_index = list(range(f_index[0], f_index[1] + 1))\n",
    "    \n",
    "    g_index = np.digitize(g_index, j_bins, right=True)\n",
    "    g_index = list(range(g_index[0], g_index[1] + 1))\n",
    "    \n",
    "    h_index = np.digitize(h_index, j_bins, right=True)\n",
    "    h_index = list(range(h_index[0], h_index[1] + 1))\n",
    "    \n",
    "    i_index = np.digitize(i_index, j_bins, right=True)\n",
    "    i_index = list(range(i_index[0], i_index[1] + 1))\n",
    "    \n",
    "    j_index = np.digitize(j_index, j_bins, right=True)\n",
    "    j_index = list(range(j_index[0], j_index[1] + 1))\n",
    "    \n",
    "    k_index = np.digitize(k_index, j_bins, right=True)\n",
    "    k_index = list(range(k_index[0], k_index[1] + 1))\n",
    "    \n",
    "    l_index = np.digitize(l_index, j_bins, right=True)\n",
    "    l_index = list(range(l_index[0], l_index[1] + 1))\n",
    "    \n",
    "    m_index = np.digitize(m_index, j_bins, right=True)\n",
    "    m_index = list(range(m_index[0], m_index[1] + 1))\n",
    "    \n",
    "    # Calculating the sum of elements in each J bins\n",
    "    # Initialize the output array with zeros\n",
    "    x_j = [0] * len(j_idx)\n",
    "    \n",
    "    # Count occurrences in a_index\n",
    "    for index in a_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in b_index\n",
    "    for index in b_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in c_index\n",
    "    for index in c_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in d_index\n",
    "    for index in d_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in e_index\n",
    "    for index in e_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in f_index\n",
    "    for index in f_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in g_index\n",
    "    for index in g_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in h_index\n",
    "    for index in h_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in i_index\n",
    "    for index in i_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in j_index\n",
    "    for index in j_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in k_index\n",
    "    for index in k_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in l_index\n",
    "    for index in l_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    # Count occurrences in m_index\n",
    "    for index in m_index:\n",
    "        if index in j_idx:\n",
    "            x_j[index] += 1\n",
    "    \n",
    "    x_j = x_j[1:]\n",
    "    #print(\"Output Array:\", x_j)\n",
    "    #print(\"Output Array created!\")\n",
    "    \n",
    "    '''Validation'''\n",
    "    #print(\"Validation: \" + str(len(x_j) == todayRange))\n",
    "    \n",
    "    '''Calculate the VAH, VAL and POC'''\n",
    "    # Calculate the half price index\n",
    "    halfpriceIndex = int(todayRange/2)\n",
    "    \n",
    "    # Calculate sum of lower and upper count\n",
    "    lowerCount = sum(x_j[0:halfpriceIndex])\n",
    "    upperCount = sum(x_j[halfpriceIndex:])\n",
    "    tpoHighorLow = 1 if lowerCount > upperCount else 2 if lowerCount < upperCount else -1\n",
    "    \n",
    "    # Calculate the POC\n",
    "    maxTPOCounter = x_j.count(max(x_j))\n",
    "    maxTPOArray = [i for i in range(len(x_j)) if x_j[i] == max(x_j)]\n",
    "    \n",
    "    if maxTPOCounter == 1:\n",
    "        maxJ = maxTPOArray[0]\n",
    "    else:\n",
    "        maxTPOdistfromMidArray = [abs(x - halfpriceIndex) for x in maxTPOArray]\n",
    "        minDistCount = maxTPOdistfromMidArray.count(min(maxTPOdistfromMidArray))\n",
    "        minDistArray = [maxTPOArray[i] for i in [i for i in range(len(maxTPOdistfromMidArray)) if maxTPOdistfromMidArray[i] == min(maxTPOdistfromMidArray)]]\n",
    "        if minDistCount == 1:\n",
    "            maxJ = minDistArray[0]\n",
    "        else:\n",
    "            maxJ = minDistArray[0] if tpoHighorLow == 1 or tpoHighorLow == -1 else minDistArray[1]\n",
    "    \n",
    "    # Calculate the VAH and VAL\n",
    "    up_idx, down_idx = find_indexes(x_j, maxJ)\n",
    "    vah = round(baseY + (up_idx * Den),2)\n",
    "    val = round(baseY + (down_idx * Den),2)\n",
    "    poc = round(baseY + (maxJ * Den),2)\n",
    "    \n",
    "    #print(f\"VAL: {val}, POC: {poc}, VAH:{vah}, maxJ:{maxJ}, up:{up_idx}, dn:{down_idx}, base:{baseY}, Den:{Den}\")\n",
    "    \n",
    "    '''Calculate IBH, IBL and IBType'''\n",
    "    # First 60mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min60_filter = df_15min_temp.collect()[0:4]\n",
    "        first60minHigh = [ ele.__getattr__(\"High\") for ele in min60_filter]\n",
    "        first60minHigh.sort(reverse=True)\n",
    "        first60minHigh = round(list(map(first60minHigh.__getitem__, [0]))[0], 2)\n",
    "        first60minLow = [ ele.__getattr__(\"Low\") for ele in min60_filter]\n",
    "        first60minLow.sort()\n",
    "        first60minLow = round(list(map(first60minLow.__getitem__, [0]))[0], 2)\n",
    "        first60minOpen = [ ele.__getattr__(\"Open\") for ele in min60_filter]    \n",
    "        first60minOpen = round(list(map(first60minOpen.__getitem__, [0]))[0], 2)\n",
    "        first60minClose = [ ele.__getattr__(\"Close\") for ele in min60_filter]    \n",
    "        first60minClose = round(list(map(first60minClose.__getitem__, [3]))[0], 2)\n",
    "    else:\n",
    "        return\n",
    "            \n",
    "    IBH = first60minHigh\n",
    "    IBL = first60minLow\n",
    "    IBRange = round(IBH - IBL, 2)\n",
    "    \n",
    "    IBTarget0High = IBH + (IBRange * 0.25)\n",
    "    IBTarget0Low = IBL - (IBRange * 0.25)\n",
    "    IBTarget1High = IBH + (IBRange * 0.5)\n",
    "    IBTarget1Low = IBL - (IBRange * 0.5)\n",
    "    IBTarget15High = IBH + (IBRange * 0.6)\n",
    "    IBTarget15Low = IBL - (IBRange * 0.6)\n",
    "    IBTarget2High = IBH + (IBRange * 0.8)\n",
    "    IBTarget2Low = IBL - (IBRange * 0.8)\n",
    "    IBTarget3High = IBH + (IBRange * 1)\n",
    "    IBTarget3Low = IBL - (IBRange * 1)\n",
    "    IBTarget4High = IBH + (IBRange * 1.2)\n",
    "    IBTarget4Low = IBL - (IBRange * 1.2)\n",
    "    \n",
    "    IBType = IBTypeCheck(IBRange, ATR)\n",
    "    #print(\"IB levels calculated!\")\n",
    "    \n",
    "    '''Calculate First5, First15, First30, Second15, Second30 and Day OHLC'''\n",
    "    dayHigh = round(getattr(day_filter, \"High\"), 2)\n",
    "    dayLow =  round(getattr(day_filter, \"Low\"), 2)\n",
    "    dayOpen = round(getattr(day_filter, \"Open\"), 2)\n",
    "    dayClose = round(getattr(day_filter, \"Close\"), 2)\n",
    "    \n",
    "    \n",
    "    # First 5mins\n",
    "    if df_5min_temp.count() == 75:\n",
    "        min5_filter = df_5min_temp.collect()[0]\n",
    "        first5minHigh = round(getattr(min5_filter, \"High\"), 2)\n",
    "        first5minLow = round(getattr(min5_filter, \"Low\"), 2)\n",
    "        first5minOpen = round(getattr(min5_filter, \"Open\"), 2)\n",
    "        first5minClose = round(getattr(min5_filter, \"Close\"), 2)\n",
    "    else:\n",
    "        first5minHigh = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.High).collect()[0].High, 2)\n",
    "        first5minLow = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.Low).collect()[0].Low, 2)\n",
    "        first5minOpen = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.Open).collect()[0].Open, 2)\n",
    "        first5minClose = round(df_5min_temp.filter((df_5min_temp.Timestamp <= date_value +\" 09:19:00\")).select(df_5min_temp.Close).collect()[0].Close, 2)\n",
    "    \n",
    "    # First 15mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min15_filter = df_15min_temp.collect()[0]\n",
    "        first15minHigh = round(getattr(min15_filter, \"High\"), 2)\n",
    "        first15minLow = round(getattr(min15_filter, \"Low\"), 2)\n",
    "        first15minOpen = round(getattr(min15_filter, \"Open\"), 2)\n",
    "        first15minClose = round(getattr(min15_filter, \"Close\"), 2)\n",
    "    else:\n",
    "        return\n",
    "            \n",
    "    # First 30mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min30_filter = df_15min_temp.collect()[0:2]\n",
    "        first30minHigh = [ ele.__getattr__(\"High\") for ele in min30_filter]\n",
    "        first30minHigh.sort(reverse=True)\n",
    "        first30minHigh = round(list(map(first30minHigh.__getitem__, [0]))[0], 2)\n",
    "        first30minLow = [ ele.__getattr__(\"Low\") for ele in min30_filter]\n",
    "        first30minLow.sort()\n",
    "        first30minLow = round(list(map(first30minLow.__getitem__, [0]))[0], 2)\n",
    "        first30minOpen = [ ele.__getattr__(\"Open\") for ele in min30_filter]\n",
    "        #first30minOpen.sort()\n",
    "        first30minOpen = round(list(map(first30minOpen.__getitem__, [0]))[0], 2)\n",
    "        first30minClose = [ ele.__getattr__(\"Close\") for ele in min30_filter]\n",
    "        #first30minClose.sort()\n",
    "        first30minClose = round(list(map(first30minClose.__getitem__, [1]))[0], 2)\n",
    "    else:\n",
    "        return\n",
    "            \n",
    "    # Second 15mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min15_2_filter = df_15min_temp.collect()[1]\n",
    "        second15minHigh = round(getattr(min15_2_filter, \"High\"), 2)\n",
    "        second15minLow = round(getattr(min15_2_filter, \"Low\"), 2)\n",
    "        second15minOpen = round(getattr(min15_2_filter, \"Open\"), 2)\n",
    "        second15minClose = round(getattr(min15_2_filter, \"Close\"), 2)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    # Second 30mins\n",
    "    if df_15min_temp.count() == 25:\n",
    "        min30_2_filter = df_15min_temp.collect()[2:4]\n",
    "        second30minHigh = [ ele.__getattr__(\"High\") for ele in min30_2_filter]\n",
    "        second30minHigh.sort(reverse=True)\n",
    "        second30minHigh = round(list(map(second30minHigh.__getitem__, [0]))[0], 2)\n",
    "        second30minLow = [ ele.__getattr__(\"Low\") for ele in min30_2_filter]\n",
    "        second30minLow.sort()\n",
    "        second30minLow = round(list(map(second30minLow.__getitem__, [0]))[0], 2)\n",
    "        second30minOpen = [ ele.__getattr__(\"Open\") for ele in min30_2_filter]\n",
    "        #second30minOpen.sort()\n",
    "        second30minOpen = round(list(map(second30minOpen.__getitem__, [0]))[0], 2)\n",
    "        second30minClose = [ ele.__getattr__(\"Close\") for ele in min30_2_filter]\n",
    "        #second30minClose.sort()\n",
    "        second30minClose = round(list(map(second30minClose.__getitem__, [1]))[0], 2)\n",
    "    else:\n",
    "        return        \n",
    "    \n",
    "    '''Calculate the Extremes, RE, Value Shift and Market Sentiment'''\n",
    "    # Extremes\n",
    "    buyExtremeCount = calculateExtremes(x_j)\n",
    "    shortExtremeCount = calculateExtremes(list(reversed(x_j)))\n",
    "    netExtremeCount = buyExtremeCount - shortExtremeCount\n",
    "    \n",
    "    buyExtremePresent = 1 if buyExtremeCount >= 2 else 0\n",
    "    shortExtremePresent = 1 if shortExtremeCount >= 2 else 0  \n",
    "    extremePresent = extremePresentCheck(buyExtremePresent, shortExtremePresent, netExtremeCount)\n",
    "    \n",
    "    # Range Extension\n",
    "    buyRangeExtension = 1 if dayHigh >= IBTarget1High else 0\n",
    "    shortRangeExtension = 1 if dayLow <= IBTarget1Low else 0\n",
    "    rangeExtension = rangeExtensionCheck(buyRangeExtension, shortRangeExtension)\n",
    "    \n",
    "    # TPO Count\n",
    "    buyTPOCount = sum(x_j[:maxJ])\n",
    "    shortTPOCount = sum(x_j[maxJ + 1:])\n",
    "    tpoCount = tpoCountCheck(buyTPOCount, shortTPOCount)\n",
    "    \n",
    "    # Value Shift\n",
    "    # 1 -> Positive, 2 -> Negative, 3 -> Neutral\n",
    "    vah = round(baseY + (up_idx * Den),2)\n",
    "    val = round(baseY + (down_idx * Den),2)\n",
    "    poc = round(baseY + (maxJ * Den),2)\n",
    "    \n",
    "    if i != 0:\n",
    "        vah_prev = getattr(prev_day_filter, \"VAH\")\n",
    "        val_prev = getattr(prev_day_filter, \"VAL\")\n",
    "        poc_prev = getattr(prev_day_filter, \"POC\")\n",
    "    else:\n",
    "        vah_prev = 999999\n",
    "        val_prev = 0\n",
    "        poc_prev = 999999\n",
    "        \n",
    "    valueShift = 1 if (vah >= vah_prev * 1.005 or val >= val_prev * 1.005) and poc > poc_prev else 2 if (val <= val_prev * 0.995 or vah <= vah_prev * 0.995) and poc < poc_prev else 3\n",
    "    \n",
    "    # Market Sentiment\n",
    "    marketSentiment = marketSentimentCheck(extremePresent, rangeExtension, tpoCount, valueShift)\n",
    "    #print(\"MP levels calculated!\")\n",
    "    \n",
    "    '''Open Location and Open Conviction'''\n",
    "    openLocation = openLocationCheck(todayOpen, yestHigh, yestLow, vah_prev, val_prev)\n",
    "    \n",
    "    # Open Conviction - 5mins\n",
    "    Buying_OD_5 = first5minOpen == first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen\n",
    "    Buying_OTD_5 = first5minOpen != first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen and first5minLow > yestHigh * 0.999\n",
    "    Buying_ORR_5 = first5minClose <= ((first5minHigh - first5minLow) * 0.3) + first5minLow and first5minOpen > first5minClose\n",
    "    Selling_OD_5 = first5minOpen == first5minHigh and first5minClose <= (first5minHigh - ((first5minHigh - first5minLow) * 0.7)) and first5minClose < first5minOpen \n",
    "    Selling_OTD_5 = first5minOpen != first5minHigh and first5minClose <= (first5minHigh - ((first5minHigh - first5minLow) * 0.7)) and first5minClose < first5minOpen and first5minHigh < yestLow * 1.001\n",
    "    Selling_ORR_5 = first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minOpen < first5minClose\n",
    "    openConviction_5 = openConviction_5minCheck(Buying_OD_5, Buying_OTD_5, Buying_ORR_5, Selling_OD_5, Selling_OTD_5, Selling_ORR_5)\n",
    "    \n",
    "    # Open Conviction - 15mins\n",
    "    Buying_OD_15 = first15minOpen == first15minLow and first15minClose >= ((first15minHigh - first15minLow) * 0.7) + first15minLow and first15minClose > first15minOpen\n",
    "    Buying_OTD_15 = first15minOpen != first15minLow and first15minClose >= ((first15minHigh - first15minLow) * 0.7) + first15minLow and first15minClose > first15minOpen and first15minLow > yestHigh\n",
    "    Buying_ORR_15 = first15minOpen > first15minClose or (first15minClose <= ((first15minHigh - first15minLow) * 0.7) + first15minLow and first15minClose > first15minOpen) or first15minLow <= yestHigh\n",
    "    Selling_OD_15 = first15minOpen == first15minHigh and first15minClose <= (first15minHigh - ((first15minHigh - first15minLow) * 0.7)) and first15minClose < first15minOpen \n",
    "    Selling_OTD_15 = first15minOpen != first15minHigh and first15minClose <= (first15minHigh - ((first15minHigh - first15minLow) * 0.7)) and first15minClose < first15minOpen and first15minHigh < yestLow\n",
    "    Selling_ORR_15 = first15minOpen < first15minClose or (first15minClose >= (first15minHigh - ((first15minHigh - first15minLow) * 0.7)) and first15minClose < first15minOpen) or first15minHigh >= yestLow\n",
    "    openConviction_15 = openConviction_15minCheck(Buying_OD_15, Buying_OTD_15, Buying_ORR_15, Selling_OD_15, Selling_OTD_15, Selling_ORR_15)\n",
    "    \n",
    "    # Open Conviction - 30mins\n",
    "    Uncertain = (first15minClose > first15minOpen and second15minOpen > second15minClose) or (first15minClose < first15minOpen and second15minOpen < second15minClose)\n",
    "    StrongUp = (first30minOpen == first30minLow and first30minClose >= ((first30minHigh - first30minLow) * 0.7) + first30minLow and first30minClose > first30minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow) or (first30minOpen == first30minLow and first15minClose > first15minOpen and second15minClose > second15minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow)\n",
    "    ModerateUp = (first30minOpen <= ((first30minHigh - first30minLow) * 0.2) + first30minLow and first30minClose >= ((first30minHigh - first30minLow) * 0.7) + first30minLow and first30minClose > first30minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow) or (first30minOpen <= ((first30minHigh - first30minLow) * 0.2) + first30minLow and first15minClose > first15minOpen and second15minClose > second15minOpen and second30minLow >= (first30minHigh - first30minLow) * 0.5 + first30minLow)\n",
    "    StrongDown = (first30minOpen == first30minHigh and first30minClose <= first30minHigh - ((first30minHigh - first30minLow) * 0.7) and first30minClose < first30minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5)) or (first30minOpen == first30minHigh and first15minClose < first15minOpen and second15minClose < second15minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5))\n",
    "    ModerateDown = (first30minOpen >= first30minHigh - ((first30minHigh - first30minLow) * 0.2) and first30minClose <= first30minHigh - ((first30minHigh - first30minLow) * 0.7) and first30minClose < first30minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5)) or (first30minOpen >= first30minHigh - ((first30minHigh - first30minLow) * 0.2) and first15minClose < first15minOpen and second15minClose < second15minOpen and second30minHigh <= first30minHigh - ((first30minHigh - first30minLow) * 0.5))\n",
    "    openConviction_30 = openConviction_30minCheck(StrongUp, ModerateUp, StrongDown, ModerateDown)\n",
    "    \n",
    "    # Open Conviction - 5mins_2\n",
    "    Buying_OD_5_2 = first5minOpen == first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen\n",
    "    Buying_OTD_5_2 = first5minOpen != first5minLow and first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minClose > first5minOpen and first5minLow > vah_prev * 0.999\n",
    "    Buying_ORR_5_2 = first5minClose <= ((first5minHigh - first5minLow) * 0.3) + first5minLow and first5minOpen > first5minClose\n",
    "    Selling_OD_5_2 = first5minOpen == first5minHigh and first5minClose <= first5minHigh - ((first5minHigh - first5minLow) * 0.7) and first5minClose < first5minOpen \n",
    "    Selling_OTD_5_2 = first5minOpen != first5minHigh and first5minClose <= first5minHigh - ((first5minHigh - first5minLow) * 0.7) and first5minClose < first5minOpen and first5minHigh < val_prev * 1.001\n",
    "    Selling_ORR_5_2 = first5minClose >= ((first5minHigh - first5minLow) * 0.7) + first5minLow and first5minOpen < first5minClose\n",
    "    openConviction_5_2 = openConviction_5min_2Check(Buying_OD_5_2, Buying_OTD_5_2, Buying_ORR_5_2, Selling_OD_5_2, Selling_OTD_5_2, Selling_ORR_5_2)\n",
    "    #print(\"Open convictions calculated!\")\n",
    "    \n",
    "    '''Insert into reference table'''\n",
    "    df_output_row = spark.createDataFrame([\n",
    "        Row(Stock_Name=symbol, Date=date_value, Den=Den, \n",
    "            VAH=vah, VAL=val, POC=poc,\n",
    "            IBH=IBH , IBL=IBL, IBType=IBType, OpenLocation=openLocation,\n",
    "            OpenConviction_5=openConviction_5, OpenConviction_15=openConviction_15,\n",
    "            OpenConviction_30=openConviction_30, OpenConviction_5_2=openConviction_5_2,\n",
    "            First5_Open=first5minOpen, First5_High=first5minHigh, First5_Low=first5minLow,First5_Close=first5minClose,\n",
    "            First15_Open=first15minOpen, First15_High=first15minHigh, First15_Low=first15minLow, First15_Close=first15minClose, \n",
    "            First30_Open=first30minOpen, First30_High=first30minHigh ,First30_Low=first30minLow , First30_Close=first30minClose,\n",
    "            Second15_Open=second15minOpen ,Second15_High=second15minHigh, Second15_Low=second15minLow ,Second15_Close=second15minClose ,\n",
    "            Second30_Open=second30minOpen, Second30_High=second30minHigh ,Second30_Low=second30minLow ,Second30_Close=second30minClose ,\n",
    "            SP_Present=0,\n",
    "            Extreme_Buy_Present=buyExtremePresent, Extreme_Buy_Count=buyExtremeCount,Extreme_Short_Present=shortExtremePresent, Extreme_Short_Count=shortExtremeCount,Extreme_Present=extremePresent, Extreme_Count=netExtremeCount,\n",
    "            RE_Present=rangeExtension, TPO_Buy_Count=buyTPOCount, TPO_Short_Count=shortTPOCount, TPO_Count=tpoCount, \n",
    "            Value_Shift=valueShift, Market_Sentiment=marketSentiment, DayRange=todayRange,\n",
    "            ATR=ATR)  ])    \n",
    "    # df_ref_levels = df_output_row\n",
    "    date_value_prev = date_value\n",
    "    df_ref_levels_to_be_appended = unionAll([df_ref_levels_to_be_appended, df_output_row])\n",
    "    \n",
    "    del x_j\n",
    "    del j_bin_list\n",
    "    del j_bins\n",
    "    del j_idx\n",
    "    del data_list\n",
    "    del a_period\n",
    "    del b_period\n",
    "    del c_period\n",
    "    del d_period\n",
    "    del e_period\n",
    "    del f_period\n",
    "    del g_period\n",
    "    del h_period\n",
    "    del i_period\n",
    "    del j_period\n",
    "    del k_period\n",
    "    del l_period\n",
    "    del m_period\n",
    "    del a_high\n",
    "    del b_high\n",
    "    del c_high\n",
    "    del d_high\n",
    "    del e_high\n",
    "    del f_high\n",
    "    del g_high\n",
    "    del h_high\n",
    "    del i_high\n",
    "    del j_high\n",
    "    del k_high\n",
    "    del l_high\n",
    "    del m_high\n",
    "    del a_low\n",
    "    del b_low\n",
    "    del c_low\n",
    "    del d_low\n",
    "    del e_low\n",
    "    del f_low\n",
    "    del g_low\n",
    "    del h_low\n",
    "    del i_low\n",
    "    del j_low\n",
    "    del k_low\n",
    "    del l_low\n",
    "    del m_low\n",
    "    del a_index\n",
    "    del b_index\n",
    "    del c_index\n",
    "    del d_index\n",
    "    del e_index\n",
    "    del f_index\n",
    "    del g_index\n",
    "    del h_index\n",
    "    del i_index\n",
    "    del j_index\n",
    "    del k_index\n",
    "    del l_index\n",
    "    del m_index\n",
    "    del df_output_row\n",
    "\n",
    "    return df_ref_levels_to_be_appended\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-02\n",
      "Garbage collector: collected 405 objects.\n",
      "2017-01-03\n",
      "Garbage collector: collected 410 objects.\n",
      "2017-01-04\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-01-05\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-06\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-09\n",
      "Garbage collector: collected 440 objects.\n",
      "2017-01-10\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-01-11\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-01-12\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-13\n",
      "Garbage collector: collected 478 objects.\n",
      "2017-01-16\n",
      "Garbage collector: collected 438 objects.\n",
      "2017-01-17\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-01-18\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-19\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-20\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-01-23\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-24\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-01-25\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-01-27\n",
      "Garbage collector: collected 468 objects.\n",
      "2017-01-30\n",
      "Garbage collector: collected 501 objects.\n",
      "2017-01-31\n",
      "Garbage collector: collected 439 objects.\n",
      "2017-02-01\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-02\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-03\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-02-06\n",
      "Garbage collector: collected 440 objects.\n",
      "2017-02-07\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-02-08\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-09\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-10\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-02-13\n",
      "Garbage collector: collected 501 objects.\n",
      "2017-02-14\n",
      "Garbage collector: collected 415 objects.\n",
      "2017-02-15\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-16\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-17\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-20\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-02-21\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-22\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-02-23\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-02-27\n",
      "Garbage collector: collected 440 objects.\n",
      "2017-02-28\n",
      "Garbage collector: collected 502 objects.\n",
      "2017-03-01\n",
      "Garbage collector: collected 439 objects.\n",
      "2017-03-02\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-03\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-03-06\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-07\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-08\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-03-09\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-10\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-14\n",
      "Garbage collector: collected 440 objects.\n",
      "2017-03-15\n",
      "Garbage collector: collected 502 objects.\n",
      "2017-03-16\n",
      "Garbage collector: collected 439 objects.\n",
      "2017-03-17\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-20\n",
      "Garbage collector: collected 440 objects.\n",
      "2017-03-21\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-03-22\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-03-23\n",
      "Garbage collector: collected 441 objects.\n",
      "2017-03-24\n",
      "Garbage collector: collected 417 objects.\n",
      "2017-03-27\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o11675.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 5040.0 failed 1 times, most recent failure: Lost task 17.0 in stage 5040.0 (TID 18347) (ROWLA-DESKTOP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m df_ref_levels_to_be_appended \u001b[38;5;241m=\u001b[39m calculateValueArea(day_filter, df_15min_temp, df_5min_temp, df_ref_levels_to_be_appended)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Write to the reference levels table and flush the append table'''\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_ref_levels_to_be_appended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m (endDate \u001b[38;5;241m-\u001b[39m startDate)\u001b[38;5;241m.\u001b[39mdays \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \n\u001b[0;32m     68\u001b[0m     df_ref_levels_to_be_appended\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39mjdbcUrl,table\u001b[38;5;241m=\u001b[39mjdbcTable_refLevels,properties \u001b[38;5;241m=\u001b[39m connectionProperties,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m     69\u001b[0m     df_ref_levels_to_be_appended \u001b[38;5;241m=\u001b[39m df_ref_levels_to_be_appended\u001b[38;5;241m.\u001b[39mfilter(df_ref_levels_to_be_appended\u001b[38;5;241m.\u001b[39mStock_Name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\sql\\dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o11675.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 5040.0 failed 1 times, most recent failure: Lost task 17.0 in stage 5040.0 (TID 18347) (ROWLA-DESKTOP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 40 more\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 55635)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python38\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Program Files\\Python38\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Program Files\\Python38\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Program Files\\Python38\\lib\\socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"c:\\Program Files\\Python38\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the stock name and date from daily table and check it in the reference table\n",
    "\n",
    "df_daily = df_daily.filter(df_daily.Stock_Name == stock)\n",
    "\n",
    "startDate = datetime.datetime(2017,1,1)\n",
    "endDate = datetime.datetime(2024,2,29)\n",
    "date_value_prev = datetime.datetime(2017,1,1)\n",
    "\n",
    "for i in range((endDate - startDate).days):\n",
    "    \n",
    "    symbol = stock\n",
    "    startDate += datetime.timedelta(days=1)\n",
    "    date_value = startDate.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    try:\n",
    "        dummy_val = df_daily.filter((df_daily.Stock_Name == symbol) & (df_daily.Date == date_value)).collect()[0]\n",
    "    except:\n",
    "        continue\n",
    "    prev_day_filter = 0\n",
    "    \n",
    "    print(date_value)\n",
    "    #print(date_value_prev)\n",
    "    \n",
    "    #Continue if the stock_date is already present in reference table\n",
    "    if df_ref_levels.filter((df_ref_levels.Stock_Name == symbol) & (df_ref_levels.Date == date_value)).count() != 0:\n",
    "        print(\"Data already present for this date! Skipping!!\")\n",
    "        date_value_prev = date_value\n",
    "        continue\n",
    "    \n",
    "    # Retrieve the previous data reference levels\n",
    "    if i != 0:\n",
    "        try:            \n",
    "            prev_day_filter = df_ref_levels.filter((df_ref_levels.Stock_Name == symbol) & (df_ref_levels.Date == date_value_prev)).collect()[0]\n",
    "        except:\n",
    "            #prev_day_filter = df_ref_levels.collect()[0]\n",
    "            df_prevday_row = spark.createDataFrame([\n",
    "            Row(Stock_Name=\"\", Date=\"\", Den=0.0, \n",
    "                VAH=0.0, VAL=0.0, POC=0.0,\n",
    "                IBH=0.0 , IBL=0.0, IBType=0.0, OpenLocation=0,\n",
    "                OpenConviction_5=0, OpenConviction_15=0,\n",
    "                OpenConviction_30=0, OpenConviction_5_2=0,\n",
    "                First5_Open=0.0, First5_High=0.0, First5_Low=0.0,First5_Close=0.0,\n",
    "                First15_Open=0.0, First15_High=0.0, First15_Low=0.0, First15_Close=0.0, \n",
    "                First30_Open=0.0, First30_High=0.0 ,First30_Low=0.0 , First30_Close=0.0,\n",
    "                Second15_Open=0.0 ,Second15_High=0.0, Second15_Low=0.0 ,Second15_Close=0.0 ,\n",
    "                Second30_Open=0.0, Second30_High=0.0 ,Second30_Low=0.0 ,Second30_Close=0.0 ,\n",
    "                SP_Present=0,\n",
    "                Extreme_Buy_Present=0, Extreme_Buy_Count=0,Extreme_Short_Present=0, Extreme_Short_Count=0,Extreme_Present=0, Extreme_Count=0,\n",
    "                RE_Present=0, TPO_Buy_Count=0, TPO_Short_Count=0, TPO_Count=0, \n",
    "                Value_Shift=0, Market_Sentiment=0, DayRange=0.0,\n",
    "                ATR=0.0) ])\n",
    "            prev_day_filter = df_prevday_row.collect()[0]\n",
    "    \n",
    "    # Filter for the current day\n",
    "    day_filter = df_output.filter((df_output.Stock_Name == symbol) & (df_output.Date == date_value)).collect()[0]  \n",
    "    \n",
    "    # Getting the high and low for each of the 30min period    \n",
    "    df_15min_temp = df_15min.filter((df_15min.Stock_Name == symbol) & (df_15min.Date == date_value)).sort(df_15min.Timestamp.asc()).select(df_15min.High, df_15min.Low, df_15min.Open, df_15min.Close, df_15min.Timestamp)\n",
    "    \n",
    "    df_5min_temp = df_5min.filter((df_5min.Stock_Name == symbol) & (df_5min.Date == date_value)).sort(df_5min.Timestamp.asc())\n",
    "    \n",
    "\n",
    "    df_ref_levels_to_be_appended = calculateValueArea(day_filter, df_15min_temp, df_5min_temp, df_ref_levels_to_be_appended)\n",
    "    \n",
    "        \n",
    "    '''Write to the reference levels table and flush the append table'''\n",
    "    if df_ref_levels_to_be_appended.count() == 10 or i == (endDate - startDate).days - 1:  \n",
    "        df_ref_levels_to_be_appended.write.jdbc(url=jdbcUrl,table=jdbcTable_refLevels,properties = connectionProperties,mode=\"append\")    \n",
    "        df_ref_levels_to_be_appended = df_ref_levels_to_be_appended.filter(df_ref_levels_to_be_appended.Stock_Name == \"NA\")\n",
    "    \n",
    "    collected = gc.collect()        \n",
    " \n",
    "    print(\"Garbage collector: collected %d objects.\" % (collected))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "value_area_level_calculation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
